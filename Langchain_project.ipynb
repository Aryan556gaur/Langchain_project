{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is link to cleaned preprocessed data https://drive.google.com/drive/folders/1c2ONCSSgnRt8dIfBOXK4fTOHsVumj3vX?usp=sharing"
      ],
      "metadata": {
        "id": "ITAoamCwRW44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/drive/folders/1c2ONCSSgnRt8dIfBOXK4fTOHsVumj3vX?usp=sharing"
      ],
      "metadata": {
        "id": "U3inTOW-RUlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -q pdfminer.six\n",
        "pip install -q faiss-cpu"
      ],
      "metadata": {
        "id": "FLNPgZ0mhJES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9db797-445a-4b68-a28c-e358a6fb2ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain openai tiktoken rapidocr-onnxruntime"
      ],
      "metadata": {
        "id": "0RGM7oJk2Zsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import getpass\n",
        "from textblob import TextBlob\n",
        "from pdfminer.high_level import extract_text\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "80rZaQ4ihg1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979be921-0f14-4b78-95c0-387ab4ba1e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.llms import openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "EwMMfJb7lkXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = getpass.getpass()"
      ],
      "metadata": {
        "id": "WnefN9OPhJAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to remove punctuation\n",
        "exclude = set(string.punctuation)\n",
        "def remove_punctuation(text):\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "# Define function to remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "def remove_citation_and_refernces(text):\n",
        "    return re.sub(r'\\[([^\\]]*)\\]', '', text)\n",
        "\n",
        "# Path to the folder containing PDF files\n",
        "pdf_folder = 'https://drive.google.com/drive/folders/1mfPteojPewXLWgXMthS15D7S-525b5CA?usp=drive_link' #@param {type:\"string\"}\n",
        "\n",
        "# Path to the folder where text files will be saved\n",
        "text_folder = '/content/drive/MyDrive/pdfs' #@param {type:\"string\"}\n",
        "\n",
        "# Create the text folder if it doesn't exist\n",
        "os.makedirs(text_folder, exist_ok=True)\n",
        "\n",
        "# Create text files for each PDF\n",
        "for filename in os.listdir(pdf_folder):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_file_path = os.path.join(pdf_folder, filename)\n",
        "        text = extract_text(pdf_file_path)\n",
        "\n",
        "        # Preprocess the text\n",
        "        text = remove_punctuation(text)\n",
        "        text = remove_stopwords(text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "        text = remove_citation_and_refernces(text)\n",
        "\n",
        "        # Correcting the text (optional)\n",
        "        textBlob = TextBlob(text)\n",
        "        text = str(textBlob.correct())\n",
        "\n",
        "        # Save the preprocessed text to a new file\n",
        "        text_filename = os.path.splitext(filename)[0] + '.txt'\n",
        "        text_file_path = os.path.join(text_folder, text_filename)\n",
        "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(text)"
      ],
      "metadata": {
        "id": "o3bHZI3OFg_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=[]\n",
        "for f in os.listdir(text_folder):\n",
        "  file_path = os.path.join(text_folder, f)\n",
        "  with open(file_path,'r') as obj:\n",
        "    l.append(obj.read())\n",
        "\n",
        "full_data='\\n'.join(l)"
      ],
      "metadata": {
        "id": "cnFBqA9hBu_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries=['What are the variety of Multimodal and Multi-modular AI Approaches to Streamline Autism Diagnosis in Young Children',\n",
        "         'What is Autism Spectrum Disorder, how it is caysed',\n",
        "         'What is the cure of Autism Spectrum Disorder',\n",
        "         'What are Stereotypical and maladaptive behaviors in Autism Spectrum, how are these detected and managed',\n",
        "         'How relevant is eye contact and how it can be used to detect Autism',\n",
        "         'How can cross country trials help in development of Machine learning based Multimodal solutions ',\n",
        "         'How early infants cry can help in the early detection of Autism ',\n",
        "         'What are various methods to detect  Atypical Pattern of Facial expression in Children',\n",
        "         'What kind of facial expressions can be used to detect Autism Disorder in children',\n",
        "         'What are methods to detect Autism from home videos',\n",
        "         'What is Still-Face Paradigm in Early Screening for High-Risk Autism Spectrum Disorder',\n",
        "         'What is West Syndrome?',\n",
        "         'What is the utility of Behavior and interaction imaging at 9 months of age predict autism/intellectual disability in high-risk infants with West syndrome']"
      ],
      "metadata": {
        "id": "XJzF7XjsKHtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=TextLoader(full_data)\n",
        "document=loader.load()\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
        "text_chunks=text_splitter.split_documents(document)\n",
        "embeddings=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "vectorstore=FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "for query in queries:\n",
        "  query = query\n",
        "  docs = vectorstore.similarity_search(query,5)\n",
        "  print(f'query: {query} \\n Top 5 similar document: {docs}')\n",
        "\n",
        "  model=openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "  model.invoke(f'summarized in parts: {[i for i in docs.split()]}')\n",
        "\n",
        "\n",
        "  model.invoke(f'fully summerized: {docs}')"
      ],
      "metadata": {
        "id": "20USp10XA1VN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}